{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install -q sentence-transformers fastapi uvicorn nest-asyncio pyngrok torch pinecone PyGithub python-frontmatter requests"
      ],
      "metadata": {
        "id": "wmpDXw8vHEo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "dQl9XuggHHfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NGROK_AUTH_TOKEN = userdata.get('NGROK_AUTH_TOKEN')\n",
        "!ngrok config add-authtoken $NGROK_AUTH_TOKEN"
      ],
      "metadata": {
        "id": "5KYu-kFCHOnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import uvicorn\n",
        "import threading\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Load embedding model\n",
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "print(\"‚úÖ Model loaded (768 dimensions)\")\n",
        "\n",
        "# Create FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "class EmbedRequest(BaseModel):\n",
        "    texts: list[str]\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "  return {\"status\": \"ready\", \"model\": \"all-mpnet-base-v2\", \"dimensions\": 768}\n",
        "@app.post(\"/embed\")\n",
        "async def embed_texts(request: EmbedRequest):\n",
        "    embeddings = model.encode(request.texts, normalize_embeddings=True).tolist()\n",
        "    return {\"embeddings\": embeddings, \"count\": len(embeddings)}\n",
        "\n",
        "# Kill old tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Create tunnel\n",
        "tunnel = ngrok.connect(8000)\n",
        "\n",
        "# FIX: Extract actual URL string (not the object)\n",
        "EMBEDDING_API_URL = tunnel.public_url  # Changed from str(public_url)\n",
        "\n",
        "print(f\"\\nüöÄ Embedding API Ready!\")\n",
        "print(f\"üì° Public URL: {EMBEDDING_API_URL}\")\n",
        "print(f\"üìö Docs: {EMBEDDING_API_URL}/docs\")\n",
        "print(f\"\\n‚ö†Ô∏è  IMPORTANT: Copy the URL above and use it in Cell 4 if needed\\n\")\n",
        "\n",
        "# Start server in background\n",
        "def run_server():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"error\")\n",
        "\n",
        "server_thread = threading.Thread(target=run_server, daemon=True)\n",
        "server_thread.start()\n",
        "\n",
        "print(\"‚úÖ Server running in background\")\n",
        "print(f\"‚úÖ EMBEDDING_API_URL = {EMBEDDING_API_URL}\")\n"
      ],
      "metadata": {
        "id": "pwCADnbHHTC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ CONFIGURATION ============\n",
        "# GitHub repo settings\n",
        "GITHUB_TOKEN = userdata.get('GITHUB_PAT')\n",
        "GITHUB_REPO = \"FairArena/FairArena-Docs\"\n",
        "DOCS_PATH = \"content/docs\"\n",
        "PINECODE_DB_API_KEY = userdata.get('PINECODE_DB_API_KEY')\n",
        "\n",
        "# Pinecone settings (Get from https://app.pinecone.io/)\n",
        "PINECONE_API_KEY = PINECODE_DB_API_KEY  # Replace with your key\n",
        "PINECONE_INDEX_NAME = \"fairarena-docs-768\"\n",
        "\n",
        "# Use embedding URL from previous cell\n",
        "print(f\"Using Embedding API: {EMBEDDING_API_URL}\")\n",
        "print(f\"Pinecone Index: {PINECONE_INDEX_NAME}\")"
      ],
      "metadata": {
        "id": "8nPJf0l9Hc4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import frontmatter\n",
        "from github import Github\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from typing import List, Dict\n",
        "import time\n",
        "\n",
        "# ============ STEP 1: Fetch All Docs from GitHub (WITH AUTH) =============\n",
        "def fetch_all_docs(repo_name: str, docs_path: str, github_token: str) -> List[Dict]:\n",
        "    \"\"\"Recursively fetch all .mdx and .md files with authentication\"\"\"\n",
        "    print(\"üì• Fetching files from GitHub (authenticated)...\")\n",
        "\n",
        "    # Use token for authentication (5000 requests/hour vs 60 without)\n",
        "    g = Github(github_token) if github_token else Github()\n",
        "    repo = g.get_repo(repo_name)\n",
        "\n",
        "    all_files = []\n",
        "\n",
        "    def traverse(path: str):\n",
        "        contents = repo.get_contents(path)\n",
        "        for item in contents:\n",
        "            if item.type == \"dir\":\n",
        "                traverse(item.path)\n",
        "            elif item.path.endswith(('.mdx', '.md')):\n",
        "                all_files.append({\n",
        "                    'path': item.path,\n",
        "                    'name': item.name,\n",
        "                    'content': item.decoded_content.decode('utf-8'),\n",
        "                    'sha': item.sha,\n",
        "                    'url': item.html_url\n",
        "                })\n",
        "                print(f\"  ‚úì {item.path}\")\n",
        "\n",
        "    traverse(docs_path)\n",
        "    return all_files\n",
        "\n",
        "# ============ STEP 2: Parse MDX Files =============\n",
        "def parse_mdx_file(file_data: Dict) -> Dict:\n",
        "    \"\"\"Extract frontmatter and clean content\"\"\"\n",
        "    try:\n",
        "        doc = frontmatter.loads(file_data['content'])\n",
        "        metadata = doc.metadata\n",
        "        content = doc.content.strip()\n",
        "    except:\n",
        "        metadata = {}\n",
        "        content = file_data['content']\n",
        "\n",
        "    # Clean path for URL\n",
        "    clean_path = file_data['path'].replace('content/docs/', '').replace('.mdx', '').replace('.md', '')\n",
        "\n",
        "    return {\n",
        "        'id': file_data['sha'][:12],  # Unique ID\n",
        "        'title': metadata.get('title', file_data['name'].replace('.mdx', '').replace('.md', '')),\n",
        "        'description': metadata.get('description', ''),\n",
        "        'content': content,\n",
        "        'file_path': file_data['path'],\n",
        "        'url': f\"https://docs.fairarena.in/{clean_path}\",\n",
        "        'github_url': file_data['url'],\n",
        "        'metadata': metadata\n",
        "    }\n",
        "\n",
        "# ============ STEP 3: Generate Embeddings =============\n",
        "def generate_embeddings_batch(texts: List[str], batch_size: int = 10) -> List[List[float]]:\n",
        "    \"\"\"Generate embeddings in batches via API\"\"\"\n",
        "    all_embeddings = []\n",
        "\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i + batch_size]\n",
        "\n",
        "        try:\n",
        "            response = requests.post(\n",
        "                f\"{EMBEDDING_API_URL}/embed\",\n",
        "                json={\"texts\": batch},\n",
        "                timeout=60\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            embeddings = response.json()['embeddings']\n",
        "            all_embeddings.extend(embeddings)\n",
        "            print(f\"  ‚úì Embedded {i + len(batch)}/{len(texts)} files\")\n",
        "            time.sleep(0.5)  # Rate limit\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚úó Error on batch {i}: {e}\")\n",
        "            # Retry once\n",
        "            time.sleep(2)\n",
        "            response = requests.post(\n",
        "                f\"{EMBEDDING_API_URL}/embed\",\n",
        "                json={\"texts\": batch},\n",
        "                timeout=60\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            embeddings = response.json()['embeddings']\n",
        "            all_embeddings.extend(embeddings)\n",
        "\n",
        "    return all_embeddings\n",
        "\n",
        "# ============ STEP 4: Upload to Pinecone =============\n",
        "def upload_to_pinecone(docs: List[Dict], embeddings: List[List[float]):\n",
        "    \"\"\"Create index and upload vectors\"\"\"\n",
        "    print(\"\\n‚òÅÔ∏è  Connecting to Pinecone...\")\n",
        "\n",
        "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "    # Create index if doesn't exist\n",
        "    if PINECONE_INDEX_NAME not in pc.list_indexes().names():\n",
        "        print(f\"  Creating index '{PINECONE_INDEX_NAME}'...\")\n",
        "        pc.create_index(\n",
        "            name=PINECONE_INDEX_NAME,\n",
        "            dimension=768,\n",
        "            metric=\"cosine\",\n",
        "            spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
        "        )\n",
        "        print(\"  ‚è≥ Waiting for index to initialize...\")\n",
        "        time.sleep(15)  # Wait for index creation\n",
        "\n",
        "    index = pc.Index(PINECONE_INDEX_NAME)\n",
        "\n",
        "    # Prepare vectors\n",
        "    vectors = []\n",
        "    for doc, embedding in zip(docs, embeddings):\n",
        "        vectors.append({\n",
        "            'id': doc['id'],\n",
        "            'values': embedding,\n",
        "            'metadata': {\n",
        "                'title': doc['title'],\n",
        "                'description': doc['description'][:500] if doc['description'] else '',\n",
        "                'content': doc['content'][:1000],  # First 1000 chars\n",
        "                'file_path': doc['file_path'],\n",
        "                'url': doc['url'],\n",
        "                'github_url': doc['github_url']\n",
        "            }\n",
        "        })\n",
        "\n",
        "    # Upload in batches (Pinecone limit: 100 vectors/request)\n",
        "    batch_size = 50\n",
        "    for i in range(0, len(vectors), batch_size):\n",
        "        batch = vectors[i:i + batch_size]\n",
        "        index.upsert(vectors=batch)\n",
        "        print(f\"  ‚úì Uploaded {min(i + batch_size, len(vectors))}/{len(vectors)} vectors\")\n",
        "        time.sleep(1)  # Small delay between batches\n",
        "\n",
        "    print(f\"\\n‚úÖ Index stats: {index.describe_index_stats()}\")\n",
        "\n",
        "# ============ RUN PIPELINE =============\n",
        "print(\"üöÄ Starting FairArena Docs Pipeline\\n\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Verify config\n",
        "if not EMBEDDING_API_URL or EMBEDDING_API_URL == \"\":\n",
        "    raise ValueError(\"‚ùå EMBEDDING_API_URL not set! Rerun Cell 3 first.\")\n",
        "if PINECONE_API_KEY == \"YOUR_PINECONE_API_KEY\":\n",
        "    raise ValueError(\"‚ùå Update PINECONE_API_KEY in Cell 4!\")\n",
        "if GITHUB_TOKEN == \"ghp_YOUR_GITHUB_TOKEN_HERE\":\n",
        "    print(\"‚ö†Ô∏è  WARNING: No GitHub token provided. Using unauthenticated (rate limited).\")\n",
        "    GITHUB_TOKEN = None\n",
        "\n",
        "# Step 1: Fetch (WITH TOKEN)\n",
        "docs_raw = fetch_all_docs(GITHUB_REPO, DOCS_PATH, GITHUB_TOKEN)\n",
        "print(f\"‚úÖ Found {len(docs_raw)} files\\n\")\n",
        "\n",
        "# Step 2: Parse\n",
        "print(\"üìù Parsing MDX files...\")\n",
        "docs_parsed = [parse_mdx_file(f) for f in docs_raw]\n",
        "print(f\"‚úÖ Parsed {len(docs_parsed)} documents\\n\")\n",
        "\n",
        "# Step 3: Generate embeddings\n",
        "print(\"üß† Generating embeddings...\")\n",
        "texts_to_embed = [f\"{doc['title']}. {doc['description']}. {doc['content']}\" for doc in docs_parsed]\n",
        "embeddings = generate_embeddings_batch(texts_to_embed)\n",
        "print(f\"‚úÖ Generated {len(embeddings)} embeddings\\n\")\n",
        "\n",
        "# Step 4: Upload to Pinecone\n",
        "upload_to_pinecone(docs_parsed, embeddings)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üéâ PIPELINE COMPLETE!\")\n",
        "print(f\"üìä Total files indexed: {len(docs_parsed)}\")\n",
        "print(f\"üìÅ Index name: {PINECONE_INDEX_NAME}\")\n",
        "print(f\"üîç Ready for semantic search!\")"
      ],
      "metadata": {
        "id": "9N2VPSmdIQ45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ FETCH SPECIALIZED FILES AND FOLDERS ============\n",
        "def fetch_multiple_file_types(repo_name: str, github_token: str, rate_limit_delay: float = 0.5) -> Dict[str, List[Dict]]:\n",
        "    \"\"\"\n",
        "    Fetch multiple file types and entire folders from repo\n",
        "    Implements rate limiting to avoid hitting GitHub API limits\n",
        "    \"\"\"\n",
        "    print(f\"üì• Fetching specialized files and folders from {repo_name}...\")\n",
        "    print(f\"   (Rate limit delay: {rate_limit_delay}s between requests)\\n\")\n",
        "\n",
        "    g = Github(github_token) if github_token else Github()\n",
        "    repo = g.get_repo(repo_name)\n",
        "\n",
        "    results = {\n",
        "        'postman': [],\n",
        "        'prisma': [],\n",
        "        'config_files': [],\n",
        "        'docker_files': [],\n",
        "        'shell_scripts': [],\n",
        "        'yaml_files': [],\n",
        "        'husky': [],\n",
        "        'github_workflows': [],\n",
        "        'vscode': []\n",
        "    }\n",
        "\n",
        "    # Define folders to fetch entirely\n",
        "    entire_folders = {\n",
        "        'postman': 'Backend/postman',\n",
        "        'prisma': 'Backend/prisma',\n",
        "        'husky': '.husky',\n",
        "        'github_workflows': '.github',\n",
        "        'vscode': '.vscode',\n",
        "    }\n",
        "\n",
        "    # File patterns to search for (excluding pnpm-lock.yaml)\n",
        "    patterns = {\n",
        "        'package.json': 'config_files',\n",
        "        'package-lock.json': 'config_files',\n",
        "        'dockerfile': 'docker_files',\n",
        "        'docker-compose': 'docker_files',\n",
        "        '.dockerignore': 'docker_files',\n",
        "        '.sh': 'shell_scripts',\n",
        "        '.yaml': 'yaml_files',\n",
        "        '.yml': 'yaml_files',\n",
        "    }\n",
        "\n",
        "    def traverse_and_fetch(path: str, results_key: str, folder_name: str = None):\n",
        "        \"\"\"Traverse a directory and fetch all files with rate limiting\"\"\"\n",
        "        try:\n",
        "            contents = repo.get_contents(path)\n",
        "            for item in contents:\n",
        "                time.sleep(rate_limit_delay)  # RATE LIMITING\n",
        "\n",
        "                if item.type == \"dir\":\n",
        "                    skip_dirs = {\n",
        "                        'node_modules', '.git', 'dist', 'build', '.next',\n",
        "                        '__pycache__', 'coverage', '.pytest_cache', '.turbo'\n",
        "                    }\n",
        "                    if item.name not in skip_dirs:\n",
        "                        traverse_and_fetch(item.path, results_key, folder_name)\n",
        "                else:\n",
        "                    try:\n",
        "                        file_content = item.decoded_content.decode('utf-8')\n",
        "                        results[results_key].append({\n",
        "                            'path': item.path,\n",
        "                            'name': item.name,\n",
        "                            'content': file_content,\n",
        "                            'sha': item.sha,\n",
        "                            'url': item.html_url,\n",
        "                            'category': folder_name or results_key\n",
        "                        })\n",
        "                        print(f\"    ‚úì {item.path}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"    ‚ö†Ô∏è  Could not read {item.path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö†Ô∏è  Error accessing {path}: {str(e)[:50]}\")\n",
        "\n",
        "    # STEP 1: Fetch entire folders\n",
        "    print(\"üìÅ Fetching entire folders:\\n\")\n",
        "    for folder_key, folder_path in entire_folders.items():\n",
        "        try:\n",
        "            print(f\"  {folder_key.upper()} ({folder_path}):\")\n",
        "            traverse_and_fetch(folder_path, folder_key, folder_key)\n",
        "            time.sleep(1)  # Delay between folder fetches\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö†Ô∏è  Folder not found: {folder_path}\\n\")\n",
        "\n",
        "    # STEP 2: Fetch specific file types across entire repo\n",
        "    print(\"\\nüìÑ Fetching specific file types:\\n\")\n",
        "\n",
        "    def traverse_for_patterns(path: str, depth: int = 0):\n",
        "        \"\"\"Traverse repo and match file patterns\"\"\"\n",
        "        if depth > 20:  # Prevent infinite recursion\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            contents = repo.get_contents(path)\n",
        "            for item in contents:\n",
        "                time.sleep(rate_limit_delay)  # RATE LIMITING\n",
        "\n",
        "                if item.type == \"dir\":\n",
        "                    skip_dirs = {\n",
        "                        'node_modules', '.git', 'dist', 'build', '.next',\n",
        "                        '__pycache__', 'coverage', '.pytest_cache', '.turbo',\n",
        "                        '.husky', '.github', '.vscode'  # Already fetched\n",
        "                    }\n",
        "                    if item.name not in skip_dirs:\n",
        "                        traverse_for_patterns(item.path, depth + 1)\n",
        "                else:\n",
        "                    file_lower = item.name.lower()\n",
        "                    file_path_lower = item.path.lower()\n",
        "\n",
        "                    # Skip pnpm-lock.yaml (too large)\n",
        "                    if 'pnpm-lock.yaml' in file_path_lower:\n",
        "                        print(f\"    ‚äò {item.path} (skipped - too large)\")\n",
        "                        continue\n",
        "\n",
        "                    # Check patterns\n",
        "                    for pattern, category in patterns.items():\n",
        "                        if pattern.lower() in file_path_lower or file_lower.endswith(pattern.lower()):\n",
        "                            try:\n",
        "                                file_content = item.decoded_content.decode('utf-8')\n",
        "                                results[category].append({\n",
        "                                    'path': item.path,\n",
        "                                    'name': item.name,\n",
        "                                    'content': file_content,\n",
        "                                    'sha': item.sha,\n",
        "                                    'url': item.html_url,\n",
        "                                    'category': category\n",
        "                                })\n",
        "                                print(f\"    ‚úì {item.path}\")\n",
        "                            except:\n",
        "                                print(f\"    ‚ö†Ô∏è  Could not read {item.path}\")\n",
        "                            break\n",
        "        except Exception as e:\n",
        "            if \"API rate limit exceeded\" in str(e):\n",
        "                print(\"\\n‚ö†Ô∏è  RATE LIMIT WARNING!\")\n",
        "                print(\"   Waiting 60 seconds before retrying...\")\n",
        "                time.sleep(60)\n",
        "                traverse_for_patterns(path, depth)\n",
        "            else:\n",
        "                print(f\"  Error at {path}: {str(e)[:50]}\")\n",
        "\n",
        "    print(\"  Scanning for config, build, and script files...\")\n",
        "    traverse_for_patterns(\"\")\n",
        "\n",
        "    # Summary\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üìä FILES COLLECTED:\")\n",
        "    print(\"=\"*70)\n",
        "    for key, items in results.items():\n",
        "        if items:\n",
        "            print(f\"  ‚Ä¢ {key.replace('_', ' ').title()}: {len(items)} files\")\n",
        "\n",
        "    total = sum(len(items) for items in results.values())\n",
        "    print(f\"\\n  TOTAL: {total} files\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Execute fetch (with 0.5s delay between requests to be safe)\n",
        "print(\"üöÄ Starting comprehensive repository scan...\\n\")\n",
        "all_specialized_files = fetch_multiple_file_types(\n",
        "    GITHUB_REPO_MAIN,\n",
        "    GITHUB_TOKEN_MAIN,\n",
        "    rate_limit_delay=0.5  # Adjust if needed: 0.3-1.0 is safe\n",
        ")\n",
        "\n",
        "# ============ COMBINE ALL FILES ============\n",
        "# Flatten all specialized files into one list\n",
        "all_repo_files = []\n",
        "for category, files in all_specialized_files.items():\n",
        "    all_repo_files.extend(files)\n",
        "\n",
        "print(f\"‚úÖ Total files to process: {len(all_repo_files)}\")\n"
      ],
      "metadata": {
        "id": "MV8MkxQpwQqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ PARSE AND EMBED SPECIALIZED FILES ============\n",
        "def parse_specialized_file(file_data: Dict) -> Dict:\n",
        "    \"\"\"Parse specialized files (configs, scripts, etc)\"\"\"\n",
        "    return {\n",
        "        'id': file_data['sha'][:12],\n",
        "        'title': file_data['path'].split('/')[-1],\n",
        "        'description': f\"File: {file_data['path']} | Type: {file_data['category']}\",\n",
        "        'content': file_data['content'][:5000],  # Limit content\n",
        "        'file_path': file_data['path'],\n",
        "        'url': file_data['url'],\n",
        "        'metadata': {\n",
        "            'category': file_data['category'],\n",
        "            'file_type': file_data['name'].split('.')[-1] if '.' in file_data['name'] else 'unknown'\n",
        "        },\n",
        "        'repo_type': 'developer'\n",
        "    }\n",
        "\n",
        "print(\"üìù Parsing specialized files...\")\n",
        "docs_specialized = [parse_specialized_file(f) for f in all_repo_files]\n",
        "print(f\"‚úÖ Parsed {len(docs_specialized)} files\\n\")\n",
        "\n",
        "print(\"üß† Generating embeddings for specialized files...\")\n",
        "texts_to_embed_specialized = [\n",
        "    f\"{doc['title']}. {doc['description']}. {doc['content'][:2000]}\"\n",
        "    for doc in docs_specialized\n",
        "]\n",
        "\n",
        "embeddings_specialized = generate_embeddings_batch(texts_to_embed_specialized)\n",
        "print(f\"‚úÖ Generated {len(embeddings_specialized)} embeddings\\n\")\n",
        "\n",
        "# Upload to same main repo index\n",
        "print(f\"üì§ Uploading to Pinecone index '{PINECONE_INDEX_NAME_MAIN}'...\")\n",
        "vectors_specialized = []\n",
        "for doc, embedding in zip(docs_specialized, embeddings_specialized):\n",
        "    vectors_specialized.append({\n",
        "        'id': f\"spec_{doc['id']}\",  # Prefix with 'spec_'\n",
        "        'values': embedding,\n",
        "        'metadata': {\n",
        "            'title': doc['title'][:100],\n",
        "            'description': doc['description'][:200],\n",
        "            'file_path': doc['file_path'],\n",
        "            'url': doc['url'],\n",
        "            'category': doc['metadata']['category'],\n",
        "            'file_type': doc['metadata']['file_type'],\n",
        "            'source': 'FairArena-Specialized'\n",
        "        }\n",
        "    })\n",
        "\n",
        "# Upload in batches\n",
        "batch_size = 50\n",
        "for i in range(0, len(vectors_specialized), batch_size):\n",
        "    batch = vectors_specialized[i:i + batch_size]\n",
        "    index_main.upsert(vectors=batch)\n",
        "    print(f\"  ‚úì Uploaded {min(i + batch_size, len(vectors_specialized))}/{len(vectors_specialized)}\")\n",
        "    time.sleep(1)\n",
        "\n",
        "print(f\"\\n‚úÖ All specialized files indexed!\")\n"
      ],
      "metadata": {
        "id": "y-9SoBBzwTp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ic4hTxLAyP3V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}